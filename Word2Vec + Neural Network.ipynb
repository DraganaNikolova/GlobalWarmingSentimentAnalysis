{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2006f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import utils\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Word2vec\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Utility\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff185e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "\n",
    "W2V_SIZE = 300\n",
    "W2V_WINDOW = 7\n",
    "#W2V_EPOCH = 32\n",
    "W2V_EPOCH = 8\n",
    "\n",
    "SEQUENCE_LENGTH = 300\n",
    "# EPOCHS = 8\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8846b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "import pandas as pd\n",
    "df = pd.read_pickle(\"preprocessed_labeled.pkl\")\n",
    "#df = df[['clean','sentiment']]\n",
    "df = df[['clean_nouns','sentiment']]\n",
    "df['sentiment'] = df['sentiment'].replace(1,0)\n",
    "df['sentiment'] = df['sentiment'].replace(2,0)\n",
    "df['sentiment'] = df['sentiment'].replace(0,1) # 1 is POSITIVE\n",
    "df['sentiment'] = df['sentiment'].replace(-1,0) # 0 is NEGATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8334c7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train/test\n",
    "df_train = df[0:40000]\n",
    "df_test = df[40000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aca438ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "#sentences = [word_tokenize(text) for text in df_train.clean]\n",
    "sentences = [word_tokenize(text) for text in df_train.clean_nouns]\n",
    "# Include unlabaled sentenced\n",
    "unlabaled = pd.read_pickle(\"preprocessed.pkl\")\n",
    "#sentences_unlabaled = [word_tokenize(text) for text in unlabaled.base] # without non words and len smaller than 2\n",
    "sentences_unlabaled = [word_tokenize(text) for text in unlabaled.clean_nouns]\n",
    "\n",
    "sentences.extend(sentences_unlabaled)\n",
    "word2vec_model = gensim.models.word2vec.Word2Vec(sentences, vector_size=W2V_SIZE, window=W2V_WINDOW, min_count=1, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d31d5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size is 100286\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2vec_model.wv)\n",
    "print(\"Vocab size is\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87a4b1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words 100291\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "# tokenizer.fit_on_texts(df_train.clean)\n",
    "# tokenizer.fit_on_texts(unlabaled.base)\n",
    "tokenizer.fit_on_texts(df_train.clean_nouns)\n",
    "tokenizer.fit_on_texts(unlabaled.clean_nouns)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Total words\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ddf1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.clean), maxlen=SEQUENCE_LENGTH)\n",
    "# x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.clean), maxlen=SEQUENCE_LENGTH)\n",
    "x_train = pad_sequences(tokenizer.texts_to_sequences(df_train.clean_nouns), maxlen=SEQUENCE_LENGTH)\n",
    "x_test = pad_sequences(tokenizer.texts_to_sequences(df_test.clean_nouns), maxlen=SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8c8532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index padded sequence and then replace after balancing\n",
    "sequences = {}\n",
    "x_train_indexed = []\n",
    "i = 0\n",
    "for sequence in x_train:\n",
    "    sequences[i] = sequence\n",
    "    x_train_indexed.append(i)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc3b80f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 40000\n",
      "72916 72916\n"
     ]
    }
   ],
   "source": [
    "# Solve imbalanced data with SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X = np.array(x_train_indexed).reshape(-1,1) # each index in a list\n",
    "y = df_train['sentiment']\n",
    "print(len(X), len(y))\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc45dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_balanced = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b91358d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert after balancing\n",
    "for index in X:\n",
    "    i = int(X[index][0])\n",
    "    sequence = sequences[i]\n",
    "    x_train_balanced.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57c37141",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_balanced = np.array(x_train_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5313ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y\n",
    "y_train = np.array(y_train).reshape(-1,1)\n",
    "y_test = np.array(df_test['sentiment']).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eebcf682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (72916, 300)\n",
      "y_train (72916, 1)\n",
      "\n",
      "x_test (3943, 300)\n",
      "y_test (3943, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train\", x_train_balanced.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print()\n",
    "print(\"x_test\", x_test.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e86e2f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100291, 300)\n"
     ]
    }
   ],
   "source": [
    "# Create embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e3c1d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Embedding Layer\n",
    "embedding_layer = layers.Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0dab4b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 300, 300)          30087300  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 300, 300)          0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               160400    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 30,247,801\n",
      "Trainable params: 160,501\n",
      "Non-trainable params: 30,087,300\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build model\n",
    "model = models.Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "#model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.add(layers.Dense(1, activation='relu'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30f874b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.1363 - accuracy: 0.8364 WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "36/36 [==============================] - 801s 23s/step - loss: 1.1363 - accuracy: 0.8364 - val_loss: 1.5665 - val_accuracy: 0.8827 - lr: 0.0100\n",
      "Epoch 2/2\n",
      "36/36 [==============================] - ETA: 0s - loss: 1.1560 - accuracy: 0.9059 WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "36/36 [==============================] - 1216s 34s/step - loss: 1.1560 - accuracy: 0.9059 - val_loss: 1.6659 - val_accuracy: 0.8848 - lr: 0.0100\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "sgd = keras.optimizers.SGD(learning_rate=0.5, momentum=0.9, nesterov=True)\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'sgd', metrics = ['accuracy'])\n",
    "     \n",
    "#model.compile(loss='binary_crossentropy',optimizer=keras.optimizers.Adam(learning_rate=1e-3),metrics=['accuracy'])\n",
    "callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n",
    "              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    #epochs=EPOCHS,\n",
    "                    epochs=2,\n",
    "                    validation_split=0.1,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c037e37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"w2v_keras.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62436001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.59%\n",
      "Loss: 1.5982820987701416\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "print(\"Loss:\",scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9de3bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9813/9813 [==============================] - 4558s 464ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "def decode_sentiment(score):\n",
    "    if score > 0.5: return 1\n",
    "    elif score <= 0.5: return 0\n",
    "\n",
    "def get_features(texts):\n",
    "    #return pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=SEQUENCE_LENGTH)\n",
    "\n",
    "def predict(features):\n",
    "    # Predict\n",
    "    scores = model.predict(np.array(features))\n",
    "    return scores\n",
    "\n",
    "# Classify unlabaled data\n",
    "unlabaled = pd.read_pickle(\"preprocessed.pkl\")\n",
    "features = []\n",
    "vectors = get_features(unlabaled.clean_nouns)\n",
    "scores = predict(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c2645fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = []\n",
    "for i in range(0,len(scores)):\n",
    "    if scores[i]<1: negatives.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f779695a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19342"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negatives) # 19342 negatives out of 313985 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0981a57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word2vec_keras_negatives.txt','w') as tfile:\n",
    "    tfile.write(str(negatives))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
