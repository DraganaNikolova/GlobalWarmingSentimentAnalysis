{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2006f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import utils\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from  nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Word2vec\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Utility\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import Counter\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff185e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "\n",
    "W2V_SIZE = 200\n",
    "W2V_WINDOW = 7\n",
    "W2V_EPOCH = 8\n",
    "\n",
    "SEQUENCE_LENGTH = 200\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8846b125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "import pandas as pd\n",
    "df = pd.read_pickle(\"preprocessed_labeled.pkl\")\n",
    "df = df[['clean_nouns','sentiment']]\n",
    "df['sentiment'] = df['sentiment'].replace(1,0)\n",
    "df['sentiment'] = df['sentiment'].replace(2,0)\n",
    "df['sentiment'] = df['sentiment'].replace(0,1) # 1 is POSITIVE\n",
    "df['sentiment'] = df['sentiment'].replace(-1,0) # 0 is NEGATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aca438ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec\n",
    "sentences = [word_tokenize(text) for text in df.clean_nouns]\n",
    "# Include unlabaled sentenced\n",
    "unlabaled = pd.read_pickle(\"preprocessed.pkl\")\n",
    "sentences_unlabaled = [word_tokenize(text) for text in unlabaled.clean_nouns]\n",
    "\n",
    "sentences.extend(sentences_unlabaled)\n",
    "word2vec_model = gensim.models.word2vec.Word2Vec(sentences, vector_size=W2V_SIZE, window=W2V_WINDOW, min_count=1, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d31d5d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size is 101104\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2vec_model.wv)\n",
    "print(\"Vocab size is\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87a4b1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words 101109\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df.clean_nouns)\n",
    "tokenizer.fit_on_texts(unlabaled.clean_nouns)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Total words\", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ddf1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pad_sequences(tokenizer.texts_to_sequences(df.clean_nouns), maxlen=SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57d63fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add vader columns\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "compound, neg, pos = [], [], []\n",
    "for text in df['clean_nouns']:\n",
    "    compound.append(sia.polarity_scores(text)['compound'])\n",
    "    neg.append(sia.polarity_scores(text)['neg'])\n",
    "    pos.append(sia.polarity_scores(text)['pos'])\n",
    "df['compound'] = compound\n",
    "df['neg'] = neg\n",
    "df['pos'] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad366fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add polarity and subjectivity\n",
    "from textblob import TextBlob\n",
    "\n",
    "polarity, subjectivity = [], []\n",
    "for text in df['clean_nouns']:\n",
    "    polarity.append(TextBlob(text).sentiment.polarity)\n",
    "    subjectivity.append(TextBlob(text).sentiment.subjectivity)\n",
    "df['polarity'] = polarity\n",
    "df['subjectivity'] = subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "139a07d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with additional features\n",
    "x_new = []\n",
    "for index, row in df.iterrows():\n",
    "    vector = x[index]\n",
    "    x_new.append(np.append(vector, [row['compound'],row['pos'],row['neg'],row['polarity'],row['subjectivity']]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b60cdc48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_new[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8c8532f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index padded sequence and then replace after balancing\n",
    "sequences = {}\n",
    "x_indexed = []\n",
    "i = 0\n",
    "for sequence in x_new:\n",
    "    sequences[i] = sequence\n",
    "    x_indexed.append(i)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f2c106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Index padded sequence and then replace after balancing\n",
    "# sequences = {}\n",
    "# x_indexed = []\n",
    "# i = 0\n",
    "# for sequence in x:\n",
    "#     sequences[i] = sequence\n",
    "#     x_indexed.append(i)\n",
    "#     i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc3b80f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43943 43943\n",
      "79906 79906\n"
     ]
    }
   ],
   "source": [
    "# Solve imbalanced data with SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X = np.array(x_indexed).reshape(-1,1) # each index in a list\n",
    "y = df['sentiment']\n",
    "print(len(X), len(y))\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc45dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_balanced = []\n",
    "features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b91358d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert after balancing\n",
    "for index in X:\n",
    "    i = int(X[index][0])\n",
    "    sequence = sequences[i][0:201]\n",
    "    features.append(sequences[i][201:])\n",
    "    x_balanced.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ae401c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (47943, 200)\n",
      "y_train (47943,)\n",
      "\n",
      "x_test (15982, 200)\n",
      "y_test (15982,)\n",
      "\n",
      "x_valid (15981, 200)\n",
      "y_valid (15981,)\n"
     ]
    }
   ],
   "source": [
    "# if not vader\n",
    "x_balanced = []\n",
    "# Invert after balancing\n",
    "for index in X:\n",
    "    i = int(X[index][0])\n",
    "    sequence = sequences[i]\n",
    "    x_balanced.append(sequence)\n",
    "x_balanced = np.array(x_balanced)\n",
    "# In the first step we will split the data in training and remaining dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(x_balanced, y, train_size=0.6, random_state = 4)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.5, random_state = 4)\n",
    "X_train = np.array(X_train)\n",
    "X_valid = np.array(X_valid)\n",
    "X_test = np.array(X_test)\n",
    "# Include validation dataset\n",
    "print(\"x_train\", X_train.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print()\n",
    "print(\"x_test\", X_test.shape)\n",
    "print(\"y_test\", y_test.shape)\n",
    "print()\n",
    "print(\"x_valid\", X_valid.shape)\n",
    "print(\"y_valid\", y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57c37141",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_balanced = np.array(x_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d50f8cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the first step we will split the data in training and remaining dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=0.7, random_state = 42)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "03d4e308",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff81f4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert after balancing\n",
    "X_train_embed, X_train_features = [],[]\n",
    "for index in X_train:\n",
    "    i = int(X[index][0])\n",
    "    sequence = sequences[i][0:200]\n",
    "    X_train_features.append(sequences[i][200:])\n",
    "    X_train_embed.append(sequence)\n",
    "    \n",
    "X_valid_embed, X_valid_features = [],[]\n",
    "for index in X_valid:\n",
    "    i = int(X[index][0])\n",
    "    sequence = sequences[i][0:200]\n",
    "    X_valid_features.append(sequences[i][200:])\n",
    "    X_valid_embed.append(sequence)\n",
    "    \n",
    "X_test_embed, X_test_features = [],[]\n",
    "for index in X_test:\n",
    "    i = int(X[index][0])\n",
    "    sequence = sequences[i][0:200]\n",
    "    X_test_features.append(sequences[i][200:])\n",
    "    X_test_embed.append(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f25a4c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embed = np.array(X_train_embed)\n",
    "X_train_features = np.array(X_train_features)\n",
    "X_valid_embed = np.array(X_valid_embed)\n",
    "X_valid_features = np.array(X_valid_features)\n",
    "X_test_embed = np.array(X_test_embed)\n",
    "X_test_features = np.array(X_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9099c2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (55934, 200)\n",
      "y_train (55934,)\n",
      "\n",
      "x_test (11986, 200)\n",
      "y_test (11986,)\n",
      "\n",
      "x_valid (11986, 200)\n",
      "y_valid (11986,)\n"
     ]
    }
   ],
   "source": [
    "# Include validation dataset\n",
    "print(\"x_train\", X_train_embed.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print()\n",
    "print(\"x_test\", X_test_embed.shape)\n",
    "print(\"y_test\", y_test.shape)\n",
    "print()\n",
    "print(\"x_valid\", X_valid_embed.shape)\n",
    "print(\"y_valid\", y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "97cfba99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train (55934, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"x_train\", X_train_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e86e2f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(101109, 200)\n"
     ]
    }
   ],
   "source": [
    "# Create embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in word2vec_model.wv:\n",
    "        embedding_matrix[i] = word2vec_model.wv[word]\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e3c1d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Embedding Layer\n",
    "embedding_layer = layers.Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52c6727a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 200)]        0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 200, 200)     20221800    ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 128)          168448      ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 133)          0           ['lstm[0][0]',                   \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           8576        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            65          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 20,398,889\n",
      "Trainable params: 177,089\n",
      "Non-trainable params: 20,221,800\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "input1 = layers.Input(shape=(SEQUENCE_LENGTH,))\n",
    "meta_input = layers.Input(shape=(5,))\n",
    "emb = embedding_layer(input1)\n",
    "lstm = layers.LSTM(128)(emb)\n",
    "x = layers.Concatenate()([lstm, meta_input])\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = models.Model(inputs=[input1 , meta_input], outputs=[x])\n",
    "model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=1e-3), metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06d689b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "437/437 [==============================] - ETA: 0s - loss: 0.6669 - accuracy: 0.5794WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "437/437 [==============================] - 267s 606ms/step - loss: 0.6669 - accuracy: 0.5794 - val_loss: 0.6579 - val_accuracy: 0.5893 - lr: 0.0010\n",
      "Epoch 2/10\n",
      "437/437 [==============================] - ETA: 0s - loss: 0.6501 - accuracy: 0.6034WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "437/437 [==============================] - 255s 585ms/step - loss: 0.6501 - accuracy: 0.6034 - val_loss: 0.6506 - val_accuracy: 0.6003 - lr: 0.0010\n",
      "Epoch 3/10\n",
      "437/437 [==============================] - ETA: 0s - loss: 0.6369 - accuracy: 0.6185WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "437/437 [==============================] - 261s 597ms/step - loss: 0.6369 - accuracy: 0.6185 - val_loss: 0.6498 - val_accuracy: 0.6006 - lr: 0.0010\n",
      "Epoch 4/10\n",
      "437/437 [==============================] - ETA: 0s - loss: 0.6238 - accuracy: 0.6339WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "437/437 [==============================] - 277s 635ms/step - loss: 0.6238 - accuracy: 0.6339 - val_loss: 0.6456 - val_accuracy: 0.6009 - lr: 0.0010\n",
      "Epoch 5/10\n",
      "437/437 [==============================] - ETA: 0s - loss: 0.6079 - accuracy: 0.6518WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "437/437 [==============================] - 255s 584ms/step - loss: 0.6079 - accuracy: 0.6518 - val_loss: 0.6547 - val_accuracy: 0.5967 - lr: 0.0010\n",
      "Epoch 6/10\n",
      "437/437 [==============================] - ETA: 0s - loss: 0.5922 - accuracy: 0.6671WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "437/437 [==============================] - 253s 580ms/step - loss: 0.5922 - accuracy: 0.6671 - val_loss: 0.6512 - val_accuracy: 0.6011 - lr: 0.0010\n",
      "Epoch 7/10\n",
      "437/437 [==============================] - ETA: 0s - loss: 0.5744 - accuracy: 0.6811WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "437/437 [==============================] - 254s 582ms/step - loss: 0.5744 - accuracy: 0.6811 - val_loss: 0.6881 - val_accuracy: 0.5969 - lr: 0.0010\n",
      "Epoch 8/10\n",
      "437/437 [==============================] - ETA: 0s - loss: 0.5333 - accuracy: 0.7225WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "437/437 [==============================] - 298s 683ms/step - loss: 0.5333 - accuracy: 0.7225 - val_loss: 0.6811 - val_accuracy: 0.6011 - lr: 1.0000e-04\n",
      "Epoch 9/10\n",
      "437/437 [==============================] - ETA: 0s - loss: 0.5210 - accuracy: 0.7295WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "437/437 [==============================] - 274s 627ms/step - loss: 0.5210 - accuracy: 0.7295 - val_loss: 0.6897 - val_accuracy: 0.5985 - lr: 1.0000e-04\n",
      "Epoch 10/10\n",
      "437/437 [==============================] - ETA: 0s - loss: 0.5137 - accuracy: 0.7347WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "437/437 [==============================] - 285s 652ms/step - loss: 0.5137 - accuracy: 0.7347 - val_loss: 0.6947 - val_accuracy: 0.6010 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=3),\n",
    "              EarlyStopping(monitor='val_acc', min_delta=1e-3, patience=3)]\n",
    "history = model.fit(x=[X_train_embed, X_train_features], y=y_train, batch_size=128, epochs=10, verbose=1,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data=([X_valid_embed, X_valid_features], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce754f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 59.04%\n",
      "Loss: 0.7153916954994202\n",
      "375/375 [==============================] - 24s 63ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.52      0.56      5998\n",
      "           1       0.58      0.66      0.62      5988\n",
      "\n",
      "    accuracy                           0.59     11986\n",
      "   macro avg       0.59      0.59      0.59     11986\n",
      "weighted avg       0.59      0.59      0.59     11986\n",
      "\n",
      "0.1826172266275372\n",
      "0.5883720481941733\n",
      "0.5922123452527127\n",
      "[0.60550815 0.57891654]\n",
      "0.5904137384472179\n",
      "[0.52050684 0.66032064]\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate([X_test_embed, X_test_features], y_test, batch_size=BATCH_SIZE, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "print(\"Loss:\",scores[0])\n",
    "\n",
    "scores = model.predict([X_test_embed, X_test_features])\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "y_pred = []\n",
    "for score in scores:\n",
    "    if score >= 0.5: y_pred.append(1)\n",
    "    else: y_pred.append(0)\n",
    "\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(matthews_corrcoef(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred, average='macro'))\n",
    "\n",
    "print(precision_score(y_test, y_pred, average='macro'))\n",
    "print(precision_score(y_test, y_pred, average=None))\n",
    "\n",
    "print(recall_score(y_test, y_pred, average='macro'))\n",
    "print(recall_score(y_test, y_pred, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca2f3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca8b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict([X_test_embed, X_test_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c9de3bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "def decode_sentiment(score):\n",
    "    if score > 0.5: return 1\n",
    "    elif score <= 0.5: return 0\n",
    "\n",
    "def get_features(texts):\n",
    "    return pad_sequences(tokenizer.texts_to_sequences(texts), maxlen=SEQUENCE_LENGTH)\n",
    "\n",
    "def predict(features):\n",
    "    # Predict\n",
    "    scores = model.predict(np.array(features))\n",
    "    return scores\n",
    "\n",
    "def get_vader(texts):\n",
    "    features = []\n",
    "    for i in texts:\n",
    "        l = []\n",
    "        l.append(sia.polarity_scores(i)['compound'])\n",
    "        l.append(sia.polarity_scores(i)['neg'])\n",
    "        l.append(sia.polarity_scores(i)['pos'])\n",
    "        l.append(TextBlob(i).sentiment.polarity)\n",
    "        l.append(TextBlob(i).sentiment.subjectivity)\n",
    "        features.append(l)\n",
    "    # Create the pandas DataFrame\n",
    "    features = pd.DataFrame(features)\n",
    "    # specifying column names\n",
    "    features.columns = ['compound', 'neg', 'pos', 'polarity', 'subjectivity'] \n",
    "    return features\n",
    "        \n",
    "# Classify unlabaled data\n",
    "unlabaled = pd.read_pickle(\"preprocessed.pkl\")\n",
    "features = []\n",
    "vectors = get_features(unlabaled.clean_nouns)\n",
    "features = np.array(get_vader(unlabaled.clean_nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9ddec70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(313985, 200) (313985, 5)\n"
     ]
    }
   ],
   "source": [
    "print(vectors.shape,features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5645c8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9813/9813 [==============================] - 839s 86ms/step\n"
     ]
    }
   ],
   "source": [
    "scores = model.predict([vectors, features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c2645fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = []\n",
    "for i in range(0,len(scores)):\n",
    "    if scores[i]<0.5: negatives.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f779695a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92373"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negatives) # 92373 negatives out of 313985 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0981a57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('word2vec_keras_negatives.txt','w') as tfile:\n",
    "    tfile.write(str(negatives))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
