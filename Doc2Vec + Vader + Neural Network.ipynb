{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b1bb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(\"preprocessed_labeled.pkl\")\n",
    "#df = df[['clean','sentiment']]\n",
    "\n",
    "# test with stemmed\n",
    "df = df[['clean_nouns','sentiment']]\n",
    "\n",
    "# 2(News): the tweet links to factual news about climate change\n",
    "# 1(Pro): the tweet supports the belief of man-made climate change\n",
    "# 0(Neutral: the tweet neither supports nor refutes the belief of man-made climate change\n",
    "# -1(Anti): the tweet does not believe in man-made climate change, 3990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ceddedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make positive, fact and neutral to be 0\n",
    "df['sentiment'] = df['sentiment'].replace(1,0)\n",
    "df['sentiment'] = df['sentiment'].replace(2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c863330",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment'].replace(0,1) # 1 is POSITIVE\n",
    "df['sentiment'] = df['sentiment'].replace(-1,0) # 0 is NEGATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9520781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include all negatives\n",
    "negatives = df[df.index.isin(range(40000,50000))]\n",
    "negatives = negatives[negatives['sentiment']==0]\n",
    "df_1 = df[0:40000]\n",
    "df_2 = df[40000:]\n",
    "frames = [df_1,negatives,df_2]\n",
    "df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc55baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42745f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f745911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add vader columns\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "compound, neg, pos = [], [], []\n",
    "#for text in df['clean']:\n",
    "for text in df['clean_nouns']:\n",
    "    compound.append(sia.polarity_scores(text)['compound'])\n",
    "    neg.append(sia.polarity_scores(text)['neg'])\n",
    "    pos.append(sia.polarity_scores(text)['pos'])\n",
    "df['compound'] = compound\n",
    "df['neg'] = neg\n",
    "df['pos'] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fe53f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add polarity and subjectivity\n",
    "from textblob import TextBlob\n",
    "\n",
    "polarity, subjectivity = [], []\n",
    "#for text in df['clean']:\n",
    "for text in df['clean_nouns']:\n",
    "    polarity.append(TextBlob(text).sentiment.polarity)\n",
    "    subjectivity.append(TextBlob(text).sentiment.subjectivity)\n",
    "df['polarity'] = polarity\n",
    "df['subjectivity'] = subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1d74708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4438"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['sentiment']==0]) # 4438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e6de2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# List of sentences\n",
    "#doc = df[\"clean\"]\n",
    "doc = df[\"clean_nouns\"]\n",
    "# Tokenization of each document\n",
    "tokenized_doc = []\n",
    "for d in doc:\n",
    "    tokenized_doc.append(word_tokenize(d.lower()))\n",
    "    \n",
    "# Add unlabaled documents\n",
    "# unlabeled = pd.read_pickle(\"preprocessed.pkl\")\n",
    "# doc = unlabeled[\"base\"]\n",
    "# # Tokenization of each unlabeled document\n",
    "# for d in doc:\n",
    "#     tokenized_doc.append(word_tokenize(d.lower()))\n",
    "\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_doc)]\n",
    "\n",
    "## Train doc2vec model\n",
    "d2v_model = Doc2Vec(tagged_data, vector_size = 100, window = 2, min_count = 1, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08d9dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "# d2v_model.save(\"d2v_model.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85467567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "d2v_model = Doc2Vec.load(\"d2v_model.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4af242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Doc2Vec in df_features\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df['vectors'] = df.clean_nouns.apply(lambda x: d2v_model.infer_vector(word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ac6c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add each in one vector\n",
    "features = []\n",
    "for index, row in df.iterrows():\n",
    "    featuresRow = []\n",
    "    for column in df.columns:\n",
    "        if column == 'clean' or column == 'clean_nouns' or column == 'stemmed' or column == 'sentiment': continue\n",
    "        if column == 'vectors': \n",
    "            for i in list(row[column]):\n",
    "                featuresRow.append(i)\n",
    "            continue\n",
    "        featuresRow.append(row[column])\n",
    "    features.append(featuresRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34d658cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pandas DataFrame\n",
    "df_features = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad0a7913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44391 44391\n",
      "79906 79906\n"
     ]
    }
   ],
   "source": [
    "# Solve imbalanced data with SLOVE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from random import shuffle\n",
    "\n",
    "X = features\n",
    "y = df['sentiment']\n",
    "print(len(X), len(y))\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f919964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the first step we will split the data in training and remaining dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=0.6, random_state = 4)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.5, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17fa88b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "train_x = np.array(X_train)\n",
    "train_y = np.array(y_train)\n",
    "test_x = np.array(X_test)\n",
    "test_y = np.array(y_test)\n",
    "valid_x = np.array(X_valid)\n",
    "valid_y = np.array(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d52121d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40500 40500\n",
      "73002 73002\n"
     ]
    }
   ],
   "source": [
    "# # Solve imbalanced data with SLOVE\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# X = features[0:40500]\n",
    "# y = df[0:40500]['sentiment']\n",
    "# print(len(X), len(y))\n",
    "# oversample = SMOTE()\n",
    "# X, y = oversample.fit_resample(X, y)\n",
    "# print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c0253830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_12 (Dense)            (None, 100)               10600     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 50)                5050      \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,701\n",
      "Trainable params: 15,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential\n",
    "from keras import models\n",
    "from keras import layers\n",
    "# Train a neural network\n",
    "model = models.Sequential()\n",
    "# Input - Layer\n",
    "model.add(layers.Dense(100, activation = \"relu\", input_shape=(105, )))\n",
    "# Hidden - Layers\n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "# Output- Layer\n",
    "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4ae9aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "746/750 [============================>.] - ETA: 0s - loss: 0.2470 - accuracy: 0.8997 - precision_7: 0.9146 - recall_7: 0.8816 - auc_7: 0.9622WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,precision_7,recall_7,auc_7,val_loss,val_accuracy,val_precision_7,val_recall_7,val_auc_7,lr\n",
      "750/750 [==============================] - 10s 8ms/step - loss: 0.2469 - accuracy: 0.8998 - precision_7: 0.9147 - recall_7: 0.8818 - auc_7: 0.9622 - val_loss: 0.2551 - val_accuracy: 0.9006 - val_precision_7: 0.9206 - val_recall_7: 0.8772 - val_auc_7: 0.9596 - lr: 0.0010\n",
      "Epoch 2/20\n",
      "739/750 [============================>.] - ETA: 0s - loss: 0.2349 - accuracy: 0.9051 - precision_7: 0.9189 - recall_7: 0.8886 - auc_7: 0.9656WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,precision_7,recall_7,auc_7,val_loss,val_accuracy,val_precision_7,val_recall_7,val_auc_7,lr\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.2348 - accuracy: 0.9053 - precision_7: 0.9190 - recall_7: 0.8890 - auc_7: 0.9656 - val_loss: 0.2459 - val_accuracy: 0.9056 - val_precision_7: 0.9307 - val_recall_7: 0.8769 - val_auc_7: 0.9624 - lr: 0.0010\n",
      "Epoch 3/20\n",
      "747/750 [============================>.] - ETA: 0s - loss: 0.2295 - accuracy: 0.9088 - precision_7: 0.9231 - recall_7: 0.8918 - auc_7: 0.9672WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,precision_7,recall_7,auc_7,val_loss,val_accuracy,val_precision_7,val_recall_7,val_auc_7,lr\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.2295 - accuracy: 0.9087 - precision_7: 0.9231 - recall_7: 0.8918 - auc_7: 0.9672 - val_loss: 0.2418 - val_accuracy: 0.9080 - val_precision_7: 0.9222 - val_recall_7: 0.8915 - val_auc_7: 0.9630 - lr: 0.0010\n",
      "Epoch 4/20\n",
      "748/750 [============================>.] - ETA: 0s - loss: 0.2229 - accuracy: 0.9110 - precision_7: 0.9241 - recall_7: 0.8955 - auc_7: 0.9692WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,precision_7,recall_7,auc_7,val_loss,val_accuracy,val_precision_7,val_recall_7,val_auc_7,lr\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.2230 - accuracy: 0.9109 - precision_7: 0.9242 - recall_7: 0.8953 - auc_7: 0.9692 - val_loss: 0.2381 - val_accuracy: 0.9111 - val_precision_7: 0.9204 - val_recall_7: 0.9005 - val_auc_7: 0.9644 - lr: 0.0010\n",
      "Epoch 5/20\n",
      "748/750 [============================>.] - ETA: 0s - loss: 0.2177 - accuracy: 0.9139 - precision_7: 0.9277 - recall_7: 0.8978 - auc_7: 0.9704WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,precision_7,recall_7,auc_7,val_loss,val_accuracy,val_precision_7,val_recall_7,val_auc_7,lr\n",
      "750/750 [==============================] - 4s 6ms/step - loss: 0.2177 - accuracy: 0.9139 - precision_7: 0.9277 - recall_7: 0.8978 - auc_7: 0.9704 - val_loss: 0.2325 - val_accuracy: 0.9116 - val_precision_7: 0.9368 - val_recall_7: 0.8832 - val_auc_7: 0.9661 - lr: 0.0010\n",
      "Epoch 6/20\n",
      "750/750 [==============================] - ETA: 0s - loss: 0.2117 - accuracy: 0.9165 - precision_7: 0.9299 - recall_7: 0.9010 - auc_7: 0.9720WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,precision_7,recall_7,auc_7,val_loss,val_accuracy,val_precision_7,val_recall_7,val_auc_7,lr\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.2117 - accuracy: 0.9165 - precision_7: 0.9299 - recall_7: 0.9010 - auc_7: 0.9720 - val_loss: 0.2286 - val_accuracy: 0.9141 - val_precision_7: 0.9328 - val_recall_7: 0.8929 - val_auc_7: 0.9668 - lr: 0.0010\n",
      "Epoch 7/20\n",
      "741/750 [============================>.] - ETA: 0s - loss: 0.2052 - accuracy: 0.9186 - precision_7: 0.9335 - recall_7: 0.9014 - auc_7: 0.9736WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,precision_7,recall_7,auc_7,val_loss,val_accuracy,val_precision_7,val_recall_7,val_auc_7,lr\n",
      "750/750 [==============================] - 4s 6ms/step - loss: 0.2050 - accuracy: 0.9187 - precision_7: 0.9337 - recall_7: 0.9014 - auc_7: 0.9737 - val_loss: 0.2239 - val_accuracy: 0.9152 - val_precision_7: 0.9294 - val_recall_7: 0.8991 - val_auc_7: 0.9683 - lr: 0.0010\n",
      "Epoch 8/20\n",
      "747/750 [============================>.] - ETA: 0s - loss: 0.2033 - accuracy: 0.9201 - precision_7: 0.9328 - recall_7: 0.9055 - auc_7: 0.9740WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,precision_7,recall_7,auc_7,val_loss,val_accuracy,val_precision_7,val_recall_7,val_auc_7,lr\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.2033 - accuracy: 0.9201 - precision_7: 0.9327 - recall_7: 0.9055 - auc_7: 0.9740 - val_loss: 0.2205 - val_accuracy: 0.9174 - val_precision_7: 0.9275 - val_recall_7: 0.9060 - val_auc_7: 0.9688 - lr: 0.0010\n",
      "Epoch 9/20\n",
      "744/750 [============================>.] - ETA: 0s - loss: 0.1988 - accuracy: 0.9235 - precision_7: 0.9376 - recall_7: 0.9074 - auc_7: 0.9750WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,precision_7,recall_7,auc_7,val_loss,val_accuracy,val_precision_7,val_recall_7,val_auc_7,lr\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.1988 - accuracy: 0.9236 - precision_7: 0.9377 - recall_7: 0.9074 - auc_7: 0.9750 - val_loss: 0.2221 - val_accuracy: 0.9163 - val_precision_7: 0.9441 - val_recall_7: 0.8854 - val_auc_7: 0.9687 - lr: 0.0010\n",
      "Epoch 10/20\n",
      "750/750 [==============================] - ETA: 0s - loss: 0.1921 - accuracy: 0.9254 - precision_7: 0.9383 - recall_7: 0.9107 - auc_7: 0.9768WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,precision_7,recall_7,auc_7,val_loss,val_accuracy,val_precision_7,val_recall_7,val_auc_7,lr\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.1921 - accuracy: 0.9254 - precision_7: 0.9383 - recall_7: 0.9107 - auc_7: 0.9768 - val_loss: 0.2210 - val_accuracy: 0.9173 - val_precision_7: 0.9451 - val_recall_7: 0.8865 - val_auc_7: 0.9693 - lr: 0.0010\n",
      "Epoch 11/20\n",
      "742/750 [============================>.] - ETA: 0s - loss: 0.1898 - accuracy: 0.9256 - precision_7: 0.9378 - recall_7: 0.9116 - auc_7: 0.9773WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,precision_7,recall_7,auc_7,val_loss,val_accuracy,val_precision_7,val_recall_7,val_auc_7,lr\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.1897 - accuracy: 0.9258 - precision_7: 0.9379 - recall_7: 0.9120 - auc_7: 0.9773 - val_loss: 0.2150 - val_accuracy: 0.9204 - val_precision_7: 0.9461 - val_recall_7: 0.8920 - val_auc_7: 0.9701 - lr: 0.0010\n",
      "Epoch 12/20\n",
      "739/750 [============================>.] - ETA: 0s - loss: 0.1849 - accuracy: 0.9292 - precision_7: 0.9412 - recall_7: 0.9156 - auc_7: 0.9783WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,precision_7,recall_7,auc_7,val_loss,val_accuracy,val_precision_7,val_recall_7,val_auc_7,lr\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.1848 - accuracy: 0.9292 - precision_7: 0.9413 - recall_7: 0.9155 - auc_7: 0.9783 - val_loss: 0.2141 - val_accuracy: 0.9213 - val_precision_7: 0.9484 - val_recall_7: 0.8914 - val_auc_7: 0.9707 - lr: 0.0010\n",
      "Epoch 13/20\n",
      "739/750 [============================>.] - ETA: 0s - loss: 0.1829 - accuracy: 0.9282 - precision_7: 0.9411 - recall_7: 0.9134 - auc_7: 0.9789WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,precision_7,recall_7,auc_7,val_loss,val_accuracy,val_precision_7,val_recall_7,val_auc_7,lr\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.1824 - accuracy: 0.9284 - precision_7: 0.9414 - recall_7: 0.9137 - auc_7: 0.9790 - val_loss: 0.2137 - val_accuracy: 0.9216 - val_precision_7: 0.9458 - val_recall_7: 0.8949 - val_auc_7: 0.9704 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20\n",
      "739/750 [============================>.] - ETA: 0s - loss: 0.1786 - accuracy: 0.9319 - precision_7: 0.9455 - recall_7: 0.9167 - auc_7: 0.9796WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,precision_7,recall_7,auc_7,val_loss,val_accuracy,val_precision_7,val_recall_7,val_auc_7,lr\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.1787 - accuracy: 0.9319 - precision_7: 0.9455 - recall_7: 0.9166 - auc_7: 0.9796 - val_loss: 0.2110 - val_accuracy: 0.9237 - val_precision_7: 0.9458 - val_recall_7: 0.8992 - val_auc_7: 0.9707 - lr: 0.0010\n",
      "Epoch 15/20\n",
      "737/750 [============================>.] - ETA: 0s - loss: 0.1721 - accuracy: 0.9338 - precision_7: 0.9449 - recall_7: 0.9214 - auc_7: 0.9811WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,precision_7,recall_7,auc_7,val_loss,val_accuracy,val_precision_7,val_recall_7,val_auc_7,lr\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.1720 - accuracy: 0.9339 - precision_7: 0.9449 - recall_7: 0.9216 - auc_7: 0.9811 - val_loss: 0.2095 - val_accuracy: 0.9233 - val_precision_7: 0.9574 - val_recall_7: 0.8865 - val_auc_7: 0.9724 - lr: 0.0010\n",
      "Epoch 16/20\n",
      "744/750 [============================>.] - ETA: 0s - loss: 0.1701 - accuracy: 0.9337 - precision_7: 0.9462 - recall_7: 0.9197 - auc_7: 0.9816WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,precision_7,recall_7,auc_7,val_loss,val_accuracy,val_precision_7,val_recall_7,val_auc_7,lr\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.1702 - accuracy: 0.9336 - precision_7: 0.9461 - recall_7: 0.9195 - auc_7: 0.9816 - val_loss: 0.2065 - val_accuracy: 0.9261 - val_precision_7: 0.9506 - val_recall_7: 0.8992 - val_auc_7: 0.9722 - lr: 0.0010\n",
      "Epoch 17/20\n",
      "746/750 [============================>.] - ETA: 0s - loss: 0.1709 - accuracy: 0.9349 - precision_7: 0.9472 - recall_7: 0.9213 - auc_7: 0.9812WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,precision_7,recall_7,auc_7,val_loss,val_accuracy,val_precision_7,val_recall_7,val_auc_7,lr\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.1709 - accuracy: 0.9349 - precision_7: 0.9471 - recall_7: 0.9213 - auc_7: 0.9812 - val_loss: 0.2015 - val_accuracy: 0.9272 - val_precision_7: 0.9473 - val_recall_7: 0.9050 - val_auc_7: 0.9733 - lr: 0.0010\n",
      "Epoch 18/20\n",
      "748/750 [============================>.] - ETA: 0s - loss: 0.1691 - accuracy: 0.9347 - precision_7: 0.9473 - recall_7: 0.9205 - auc_7: 0.9816WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,precision_7,recall_7,auc_7,val_loss,val_accuracy,val_precision_7,val_recall_7,val_auc_7,lr\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.1692 - accuracy: 0.9347 - precision_7: 0.9473 - recall_7: 0.9205 - auc_7: 0.9816 - val_loss: 0.2023 - val_accuracy: 0.9271 - val_precision_7: 0.9468 - val_recall_7: 0.9053 - val_auc_7: 0.9728 - lr: 0.0010\n",
      "Epoch 19/20\n",
      "741/750 [============================>.] - ETA: 0s - loss: 0.1645 - accuracy: 0.9362 - precision_7: 0.9463 - recall_7: 0.9250 - auc_7: 0.9827WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,precision_7,recall_7,auc_7,val_loss,val_accuracy,val_precision_7,val_recall_7,val_auc_7,lr\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.1652 - accuracy: 0.9360 - precision_7: 0.9459 - recall_7: 0.9250 - auc_7: 0.9826 - val_loss: 0.2092 - val_accuracy: 0.9259 - val_precision_7: 0.9617 - val_recall_7: 0.8875 - val_auc_7: 0.9726 - lr: 0.0010\n",
      "Epoch 20/20\n",
      "737/750 [============================>.] - ETA: 0s - loss: 0.1619 - accuracy: 0.9392 - precision_7: 0.9507 - recall_7: 0.9263 - auc_7: 0.9829WARNING:tensorflow:Early stopping conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,precision_7,recall_7,auc_7,val_loss,val_accuracy,val_precision_7,val_recall_7,val_auc_7,lr\n",
      "750/750 [==============================] - 4s 5ms/step - loss: 0.1620 - accuracy: 0.9391 - precision_7: 0.9507 - recall_7: 0.9261 - auc_7: 0.9828 - val_loss: 0.2005 - val_accuracy: 0.9278 - val_precision_7: 0.9422 - val_recall_7: 0.9118 - val_auc_7: 0.9733 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import keras\n",
    "\n",
    "callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=3),\n",
    "              EarlyStopping(monitor='val_acc', min_delta=1e-3, patience=3)]\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),loss = \"binary_crossentropy\",metrics = [\"accuracy\",tf.keras.metrics.Precision(),tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])\n",
    "results = model.fit(train_x, train_y, epochs= 20, batch_size = 64 ,validation_data = (valid_x, valid_y),callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "edf46109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20399333536624908, 0.9294205904006958, 0.9420495629310608, 0.9148909449577332, 0.9726414084434509]\n",
      "500/500 [==============================] - 1s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.94      0.93      8004\n",
      "           1       0.94      0.91      0.93      7978\n",
      "\n",
      "    accuracy                           0.93     15982\n",
      "   macro avg       0.93      0.93      0.93     15982\n",
      "weighted avg       0.93      0.93      0.93     15982\n",
      "\n",
      "0.859190209253083\n",
      "0.9294024844760778\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model CLEAN NOUNS\n",
    "# loss, accuracy, precision, recall, auc_roc\n",
    "scores = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(scores)\n",
    "\n",
    "scores = model.predict(test_x)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_test = test_y\n",
    "y_pred = []\n",
    "for score in scores:\n",
    "    if score >= 0.5: y_pred.append(1)\n",
    "    else: y_pred.append(0)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(matthews_corrcoef(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1cc503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"d2v_keras.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce88d24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9813/9813 [==============================] - 17s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Classify unlabaled data\n",
    "unlabaled = pd.read_pickle(\"preprocessed.pkl\")\n",
    "\n",
    "# Predict\n",
    "def decode_sentiment(score):\n",
    "    if score > 0.5: return 1\n",
    "    elif score <= 0.5: return 0 \n",
    "\n",
    "def get_features(text):\n",
    "    features = []\n",
    "    # Add features for text\n",
    "    features.append(sia.polarity_scores(text)['compound'])\n",
    "    features.append(sia.polarity_scores(text)['neg'])\n",
    "    features.append(sia.polarity_scores(text)['pos'])\n",
    "    features.append(TextBlob(text).sentiment.polarity)\n",
    "    features.append(TextBlob(text).sentiment.subjectivity)\n",
    "    vector = d2v_model.infer_vector(word_tokenize(text))\n",
    "    for i in vector:\n",
    "        features.append(i)\n",
    "    return features\n",
    "\n",
    "def predict(features):\n",
    "    scores = model.predict(np.array(features))\n",
    "    return scores\n",
    "\n",
    "negatives = []\n",
    "features = []\n",
    "for index, row in unlabaled.iterrows():\n",
    "    vector = get_features(row[\"clean_nouns\"])\n",
    "    features.append(vector)\n",
    "scores = predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9b35fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = []\n",
    "for i in range(0,len(scores)):\n",
    "    if scores[i]<0.5: negatives.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "294f8662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36823"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negatives) # 36823 negatives out of 313985 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "88b396ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('doc2vec_keras_negatives.txt','w') as tfile:\n",
    "    tfile.write(str(negatives))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
