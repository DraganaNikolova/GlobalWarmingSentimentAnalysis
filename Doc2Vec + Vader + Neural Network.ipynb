{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b1bb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(\"preprocessed_labeled.pkl\")\n",
    "df = df[['clean','sentiment']]\n",
    "# 2(News): the tweet links to factual news about climate change\n",
    "# 1(Pro): the tweet supports the belief of man-made climate change\n",
    "# 0(Neutral: the tweet neither supports nor refutes the belief of man-made climate change\n",
    "# -1(Anti): the tweet does not believe in man-made climate change, 3990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ceddedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make positive, fact and neutral to be 0\n",
    "df['sentiment'] = df['sentiment'].replace(1,0)\n",
    "df['sentiment'] = df['sentiment'].replace(2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c863330",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment'].replace(0,1) # 1 is POSITIVE\n",
    "df['sentiment'] = df['sentiment'].replace(-1,0) # 0 is NEGATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9520781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include all negatives\n",
    "negatives = df[df.index.isin(range(40000,50000))]\n",
    "negatives = negatives[negatives['sentiment']==0]\n",
    "df_1 = df[0:40000]\n",
    "df_2 = df[40000:]\n",
    "frames = [df_1,negatives,df_2]\n",
    "df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc55baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42745f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f745911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add vader columns\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "compound, neg, pos = [], [], []\n",
    "for text in df['clean']:\n",
    "    compound.append(sia.polarity_scores(text)['compound'])\n",
    "    neg.append(sia.polarity_scores(text)['neg'])\n",
    "    pos.append(sia.polarity_scores(text)['pos'])\n",
    "df['compound'] = compound\n",
    "df['neg'] = neg\n",
    "df['pos'] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7fe53f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add polarity and subjectivity\n",
    "from textblob import TextBlob\n",
    "\n",
    "polarity, subjectivity = [], []\n",
    "for text in df['clean']:\n",
    "    polarity.append(TextBlob(text).sentiment.polarity)\n",
    "    subjectivity.append(TextBlob(text).sentiment.subjectivity)\n",
    "df['polarity'] = polarity\n",
    "df['subjectivity'] = subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1d74708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4438"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['sentiment']==0]) # 4438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e6de2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# List of sentences\n",
    "doc = df[\"clean\"]\n",
    "# Tokenization of each document\n",
    "tokenized_doc = []\n",
    "for d in doc:\n",
    "    tokenized_doc.append(word_tokenize(d.lower()))\n",
    "    \n",
    "# Add unlabaled documents\n",
    "unlabeled = pd.read_pickle(\"preprocessed.pkl\")\n",
    "doc = unlabeled[\"base\"]\n",
    "# Tokenization of each unlabeled document\n",
    "for d in doc:\n",
    "    tokenized_doc.append(word_tokenize(d.lower()))\n",
    "\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_doc)]\n",
    "\n",
    "## Train doc2vec model\n",
    "d2v_model = Doc2Vec(tagged_data, vector_size = 100, window = 2, min_count = 1, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08d9dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "# d2v_model.save(\"d2v_model.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85467567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "d2v_model = Doc2Vec.load(\"d2v_model.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4af242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Doc2Vec in df_features\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df['vectors'] = df.clean.apply(lambda x: d2v_model.infer_vector(word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ac6c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add each in one vector\n",
    "features = []\n",
    "for index, row in df.iterrows():\n",
    "    featuresRow = []\n",
    "    for column in df.columns:\n",
    "        if column == 'clean' or column == 'sentiment': continue\n",
    "        if column == 'vectors': \n",
    "            for i in list(row[column]):\n",
    "                featuresRow.append(i)\n",
    "            continue\n",
    "        featuresRow.append(row[column])\n",
    "    features.append(featuresRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34d658cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pandas DataFrame\n",
    "df_features = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d52121d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40500 40500\n",
      "73002 73002\n"
     ]
    }
   ],
   "source": [
    "# Solve imbalanced data with SLOVE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "X = features[0:40500]\n",
    "y = df[0:40500]['sentiment']\n",
    "print(len(X), len(y))\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "575b2a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "train_x = np.array(X)\n",
    "train_y = np.array(y)\n",
    "test_x = np.array(features[40500:])\n",
    "test_y = np.array(df['sentiment'][40500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9a3d4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.21500000e-01,  0.00000000e+00,  3.22000000e-01, ...,\n",
       "         2.66967773e-01,  7.75308162e-02,  4.17755879e-02],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "        -2.73391753e-02,  9.31520164e-02, -1.45533934e-01],\n",
       "       [ 8.02000000e-01,  0.00000000e+00,  5.45000000e-01, ...,\n",
       "         6.28240228e-01,  3.39290112e-01,  5.51127255e-01],\n",
       "       ...,\n",
       "       [ 0.00000000e+00,  1.82861234e-01,  1.82861234e-01, ...,\n",
       "         1.08758602e-01,  1.91381307e-01,  5.92593369e-01],\n",
       "       [-5.02577589e-03,  1.61710071e-01,  6.88201330e-02, ...,\n",
       "         3.68425299e-02,  1.45305219e-01, -2.13320181e-01],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         8.01758926e-02,  4.43761046e-04,  6.12313937e-02]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2841767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 50)                5300      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 50)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,451\n",
      "Trainable params: 10,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential\n",
    "from keras import models\n",
    "from keras import layers\n",
    "# Train a neural network\n",
    "model = models.Sequential()\n",
    "# Input - Layer\n",
    "model.add(layers.Dense(50, activation = \"relu\", input_shape=(105, )))\n",
    "# Hidden - Layers\n",
    "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "# Output- Layer\n",
    "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be13486e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2282/2282 [==============================] - 6s 2ms/step - loss: 0.5059 - accuracy: 0.7552 - val_loss: 0.5062 - val_accuracy: 0.7630\n",
      "Epoch 2/10\n",
      "2282/2282 [==============================] - 5s 2ms/step - loss: 0.4267 - accuracy: 0.8097 - val_loss: 0.4443 - val_accuracy: 0.7962\n",
      "Epoch 3/10\n",
      "2282/2282 [==============================] - 4s 2ms/step - loss: 0.3942 - accuracy: 0.8270 - val_loss: 0.4420 - val_accuracy: 0.8006\n",
      "Epoch 4/10\n",
      "2282/2282 [==============================] - 4s 2ms/step - loss: 0.3721 - accuracy: 0.8375 - val_loss: 0.4665 - val_accuracy: 0.7913\n",
      "Epoch 5/10\n",
      "2282/2282 [==============================] - 4s 2ms/step - loss: 0.3587 - accuracy: 0.8460 - val_loss: 0.4433 - val_accuracy: 0.7923\n",
      "Epoch 6/10\n",
      "2282/2282 [==============================] - 5s 2ms/step - loss: 0.3495 - accuracy: 0.8518 - val_loss: 0.4495 - val_accuracy: 0.7767\n",
      "Epoch 7/10\n",
      "2282/2282 [==============================] - 5s 2ms/step - loss: 0.3372 - accuracy: 0.8573 - val_loss: 0.4505 - val_accuracy: 0.7810\n",
      "Epoch 8/10\n",
      "2282/2282 [==============================] - 7s 3ms/step - loss: 0.3328 - accuracy: 0.8601 - val_loss: 0.4729 - val_accuracy: 0.7792\n",
      "Epoch 9/10\n",
      "2282/2282 [==============================] - 7s 3ms/step - loss: 0.3261 - accuracy: 0.8623 - val_loss: 0.4196 - val_accuracy: 0.8049\n",
      "Epoch 10/10\n",
      "2282/2282 [==============================] - 5s 2ms/step - loss: 0.3217 - accuracy: 0.8652 - val_loss: 0.4480 - val_accuracy: 0.7929\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.compile(optimizer = \"adam\",loss = \"binary_crossentropy\",metrics = [\"accuracy\"])\n",
    "results = model.fit(train_x, train_y,epochs= 10,batch_size = 32,validation_data = (test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0883440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 79.29%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "scores = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "print(\"Loss:\",scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1cc503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"d2v_keras.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ce88d24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9813/9813 [==============================] - 12s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Classify unlabaled data\n",
    "unlabaled = pd.read_pickle(\"preprocessed.pkl\")\n",
    "\n",
    "# Predict\n",
    "def decode_sentiment(score):\n",
    "    if score > 0.5: return 1\n",
    "    elif score <= 0.5: return 0\n",
    "\n",
    "def get_features(text):\n",
    "    features = []\n",
    "    # Add features for text\n",
    "    features.append(sia.polarity_scores(text)['compound'])\n",
    "    features.append(sia.polarity_scores(text)['neg'])\n",
    "    features.append(sia.polarity_scores(text)['pos'])\n",
    "    features.append(TextBlob(text).sentiment.polarity)\n",
    "    features.append(TextBlob(text).sentiment.subjectivity)\n",
    "    vector = d2v_model.infer_vector(word_tokenize(text))\n",
    "    for i in vector:\n",
    "        features.append(i)\n",
    "    return features\n",
    "\n",
    "def predict(features):\n",
    "    scores = model.predict(np.array(features))\n",
    "    return scores\n",
    "\n",
    "negatives = []\n",
    "features = []\n",
    "for index, row in unlabaled.iterrows():\n",
    "    vector = get_features(row[\"clean\"])\n",
    "    features.append(vector)\n",
    "scores = predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b35fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = []\n",
    "for i in range(0,len(scores)):\n",
    "    if scores[i]<0.5: negatives.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "294f8662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62069"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negatives) # 62069 negatives out of 313985 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "88b396ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('doc2vec_keras_negatives.txt','w') as tfile:\n",
    "    tfile.write(str(negatives))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
