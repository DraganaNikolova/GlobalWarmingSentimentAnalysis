{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40b1bb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_pickle(\"preprocessed_labeled.pkl\")\n",
    "#df = df[['clean','sentiment']]\n",
    "\n",
    "# test with stemmed\n",
    "df = df[['clean_nouns','sentiment']]\n",
    "\n",
    "# 2(News): the tweet links to factual news about climate change\n",
    "# 1(Pro): the tweet supports the belief of man-made climate change\n",
    "# 0(Neutral: the tweet neither supports nor refutes the belief of man-made climate change\n",
    "# -1(Anti): the tweet does not believe in man-made climate change, 3990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ceddedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make positive, fact and neutral to be 0\n",
    "df['sentiment'] = df['sentiment'].replace(1,0)\n",
    "df['sentiment'] = df['sentiment'].replace(2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c863330",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment'] = df['sentiment'].replace(0,1) # 1 is POSITIVE\n",
    "df['sentiment'] = df['sentiment'].replace(-1,0) # 0 is NEGATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9520781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include all negatives\n",
    "negatives = df[df.index.isin(range(40000,50000))]\n",
    "negatives = negatives[negatives['sentiment']==0]\n",
    "df_1 = df[0:40000]\n",
    "df_2 = df[40000:]\n",
    "frames = [df_1,negatives,df_2]\n",
    "df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc55baa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42745f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f745911d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add vader columns\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "compound, neg, pos = [], [], []\n",
    "#for text in df['clean']:\n",
    "for text in df['clean_nouns']:\n",
    "    compound.append(sia.polarity_scores(text)['compound'])\n",
    "    neg.append(sia.polarity_scores(text)['neg'])\n",
    "    pos.append(sia.polarity_scores(text)['pos'])\n",
    "df['compound'] = compound\n",
    "df['neg'] = neg\n",
    "df['pos'] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fe53f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add polarity and subjectivity\n",
    "from textblob import TextBlob\n",
    "\n",
    "polarity, subjectivity = [], []\n",
    "#for text in df['clean']:\n",
    "for text in df['clean_nouns']:\n",
    "    polarity.append(TextBlob(text).sentiment.polarity)\n",
    "    subjectivity.append(TextBlob(text).sentiment.subjectivity)\n",
    "df['polarity'] = polarity\n",
    "df['subjectivity'] = subjectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1d74708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4438"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['sentiment']==0]) # 4438"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e6de2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# List of sentences\n",
    "#doc = df[\"clean\"]\n",
    "doc = df[\"clean_nouns\"]\n",
    "# Tokenization of each document\n",
    "tokenized_doc = []\n",
    "for d in doc:\n",
    "    tokenized_doc.append(word_tokenize(d.lower()))\n",
    "    \n",
    "# Add unlabaled documents\n",
    "# unlabeled = pd.read_pickle(\"preprocessed.pkl\")\n",
    "# doc = unlabeled[\"base\"]\n",
    "# # Tokenization of each unlabeled document\n",
    "# for d in doc:\n",
    "#     tokenized_doc.append(word_tokenize(d.lower()))\n",
    "\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(tokenized_doc)]\n",
    "\n",
    "## Train doc2vec model\n",
    "d2v_model = Doc2Vec(tagged_data, vector_size = 100, window = 2, min_count = 1, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08d9dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "# d2v_model.save(\"d2v_model.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85467567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "d2v_model = Doc2Vec.load(\"d2v_model.mod\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4af242d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Doc2Vec in df_features\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df['vectors'] = df.clean_nouns.apply(lambda x: d2v_model.infer_vector(word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ac6c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add each in one vector\n",
    "features = []\n",
    "for index, row in df.iterrows():\n",
    "    featuresRow = []\n",
    "    for column in df.columns:\n",
    "        if column == 'clean' or column == 'clean_nouns' or column == 'stemmed' or column == 'sentiment': continue\n",
    "        if column == 'vectors': \n",
    "            for i in list(row[column]):\n",
    "                featuresRow.append(i)\n",
    "            continue\n",
    "        featuresRow.append(row[column])\n",
    "    features.append(featuresRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34d658cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pandas DataFrame\n",
    "df_features = pd.DataFrame(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad0a7913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44391 44391\n",
      "79906 79906\n"
     ]
    }
   ],
   "source": [
    "# Solve imbalanced data with SLOVE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from random import shuffle\n",
    "\n",
    "X = features\n",
    "y = df['sentiment']\n",
    "print(len(X), len(y))\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f919964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the first step we will split the data in training and remaining dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_rem, y_train, y_rem = train_test_split(X, y, train_size=0.6, random_state = 4)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_rem, y_rem, test_size=0.5, random_state = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "17fa88b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "train_x = np.array(X_train)\n",
    "train_y = np.array(y_train)\n",
    "test_x = np.array(X_test)\n",
    "test_y = np.array(y_test)\n",
    "valid_x = np.array(X_valid)\n",
    "valid_y = np.array(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d52121d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40500 40500\n",
      "73002 73002\n"
     ]
    }
   ],
   "source": [
    "# # Solve imbalanced data with SLOVE\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# X = features[0:40500]\n",
    "# y = df[0:40500]['sentiment']\n",
    "# print(len(X), len(y))\n",
    "# oversample = SMOTE()\n",
    "# X, y = oversample.fit_resample(X, y)\n",
    "# print(len(X), len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "575b2a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from random import shuffle\n",
    "\n",
    "# train_x = np.array(X)\n",
    "# train_y = np.array(y)\n",
    "# test_x = np.array(features[40500:])\n",
    "# test_y = np.array(df['sentiment'][40500:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0253830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 50)                5300      \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 50)                2550      \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,901\n",
      "Trainable params: 7,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Sequential\n",
    "from keras import models\n",
    "from keras import layers\n",
    "# Train a neural network\n",
    "model = models.Sequential()\n",
    "# Input - Layer\n",
    "model.add(layers.Dense(50, activation = \"relu\", input_shape=(105, )))\n",
    "# Hidden - Layers\n",
    "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
    "model.add(layers.Dense(50, activation = \"relu\"))\n",
    "# Output- Layer\n",
    "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d533def1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "750/750 [==============================] - 14s 11ms/step - loss: 0.3019 - accuracy: 0.8715 - precision_1: 0.8839 - recall_1: 0.8553 - auc_1: 0.9436 - val_loss: 0.2964 - val_accuracy: 0.8749 - val_precision_1: 0.8831 - val_recall_1: 0.8648 - val_auc_1: 0.9455\n",
      "Epoch 2/10\n",
      "750/750 [==============================] - 6s 7ms/step - loss: 0.2931 - accuracy: 0.8773 - precision_1: 0.8888 - recall_1: 0.8625 - auc_1: 0.9469 - val_loss: 0.2950 - val_accuracy: 0.8757 - val_precision_1: 0.8990 - val_recall_1: 0.8470 - val_auc_1: 0.9463\n",
      "Epoch 3/10\n",
      "750/750 [==============================] - 5s 7ms/step - loss: 0.2898 - accuracy: 0.8791 - precision_1: 0.8898 - recall_1: 0.8653 - auc_1: 0.9480 - val_loss: 0.2918 - val_accuracy: 0.8769 - val_precision_1: 0.8908 - val_recall_1: 0.8598 - val_auc_1: 0.9470\n",
      "Epoch 4/10\n",
      "750/750 [==============================] - 5s 7ms/step - loss: 0.2854 - accuracy: 0.8816 - precision_1: 0.8926 - recall_1: 0.8675 - auc_1: 0.9497 - val_loss: 0.2884 - val_accuracy: 0.8803 - val_precision_1: 0.8874 - val_recall_1: 0.8718 - val_auc_1: 0.9486\n",
      "Epoch 5/10\n",
      "750/750 [==============================] - 6s 7ms/step - loss: 0.2839 - accuracy: 0.8815 - precision_1: 0.8942 - recall_1: 0.8654 - auc_1: 0.9500 - val_loss: 0.2856 - val_accuracy: 0.8812 - val_precision_1: 0.9040 - val_recall_1: 0.8535 - val_auc_1: 0.9493\n",
      "Epoch 6/10\n",
      "750/750 [==============================] - 5s 7ms/step - loss: 0.2800 - accuracy: 0.8838 - precision_1: 0.8952 - recall_1: 0.8693 - auc_1: 0.9516 - val_loss: 0.2847 - val_accuracy: 0.8819 - val_precision_1: 0.9027 - val_recall_1: 0.8565 - val_auc_1: 0.9498\n",
      "Epoch 7/10\n",
      "750/750 [==============================] - 5s 7ms/step - loss: 0.2776 - accuracy: 0.8844 - precision_1: 0.8977 - recall_1: 0.8677 - auc_1: 0.9523 - val_loss: 0.2828 - val_accuracy: 0.8849 - val_precision_1: 0.9069 - val_recall_1: 0.8584 - val_auc_1: 0.9504\n",
      "Epoch 8/10\n",
      "750/750 [==============================] - 6s 7ms/step - loss: 0.2744 - accuracy: 0.8864 - precision_1: 0.8985 - recall_1: 0.8711 - auc_1: 0.9534 - val_loss: 0.2793 - val_accuracy: 0.8851 - val_precision_1: 0.8971 - val_recall_1: 0.8706 - val_auc_1: 0.9512\n",
      "Epoch 9/10\n",
      "750/750 [==============================] - 5s 7ms/step - loss: 0.2723 - accuracy: 0.8881 - precision_1: 0.9009 - recall_1: 0.8721 - auc_1: 0.9542 - val_loss: 0.2790 - val_accuracy: 0.8852 - val_precision_1: 0.8927 - val_recall_1: 0.8762 - val_auc_1: 0.9515\n",
      "Epoch 10/10\n",
      "750/750 [==============================] - 5s 7ms/step - loss: 0.2683 - accuracy: 0.8896 - precision_1: 0.9021 - recall_1: 0.8740 - auc_1: 0.9553 - val_loss: 0.2749 - val_accuracy: 0.8872 - val_precision_1: 0.9004 - val_recall_1: 0.8713 - val_auc_1: 0.9526\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "import keras\n",
    "\n",
    "callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=3),\n",
    "              EarlyStopping(monitor='val_acc', min_delta=1e-3, patience=3)]\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),loss = \"binary_crossentropy\",metrics = [\"accuracy\",tf.keras.metrics.Precision(),tf.keras.metrics.Recall(), tf.keras.metrics.AUC()])\n",
    "results = model.fit(train_x, train_y, epochs= 10, batch_size = 64 ,validation_data = (valid_x, valid_y))\n",
    "                    #, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "497a59c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2732289731502533, 0.8917531967163086, 0.9038262963294983, 0.8764101266860962, 0.953108012676239]\n",
      "500/500 [==============================] - 9s 14ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.91      0.89      8004\n",
      "           1       0.90      0.88      0.89      7978\n",
      "\n",
      "    accuracy                           0.89     15982\n",
      "   macro avg       0.89      0.89      0.89     15982\n",
      "weighted avg       0.89      0.89      0.89     15982\n",
      "\n",
      "0.783854771348984\n",
      "0.8917227753791328\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model CLEAN NOUNS\n",
    "# loss, accuracy, precision, recall, auc_roc\n",
    "scores = model.evaluate(test_x, test_y, verbose=0)\n",
    "print(scores)\n",
    "\n",
    "scores = model.predict(test_x)\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "y_test = test_y\n",
    "y_pred = []\n",
    "for score in scores:\n",
    "    if score >= 0.5: y_pred.append(1)\n",
    "    else: y_pred.append(0)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "print(matthews_corrcoef(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1cc503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model.save(\"d2v_keras.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce88d24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9813/9813 [==============================] - 21s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Classify unlabaled data\n",
    "unlabaled = pd.read_pickle(\"preprocessed.pkl\")\n",
    "\n",
    "# Predict\n",
    "def decode_sentiment(score):\n",
    "    if score > 0.5: return 1\n",
    "    elif score <= 0.5: return 0\n",
    "\n",
    "def get_features(text):\n",
    "    features = []\n",
    "    # Add features for text\n",
    "    features.append(sia.polarity_scores(text)['compound'])\n",
    "    features.append(sia.polarity_scores(text)['neg'])\n",
    "    features.append(sia.polarity_scores(text)['pos'])\n",
    "    features.append(TextBlob(text).sentiment.polarity)\n",
    "    features.append(TextBlob(text).sentiment.subjectivity)\n",
    "    vector = d2v_model.infer_vector(word_tokenize(text))\n",
    "    for i in vector:\n",
    "        features.append(i)\n",
    "    return features\n",
    "\n",
    "def predict(features):\n",
    "    scores = model.predict(np.array(features))\n",
    "    return scores\n",
    "\n",
    "negatives = []\n",
    "features = []\n",
    "for index, row in unlabaled.iterrows():\n",
    "    vector = get_features(row[\"clean_nouns\"])\n",
    "    features.append(vector)\n",
    "scores = predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b35fa47",
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = []\n",
    "for i in range(0,len(scores)):\n",
    "    if scores[i]<0.5: negatives.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "294f8662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51751"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(negatives) # 62069 negatives out of 313985 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88b396ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('doc2vec_keras_negatives.txt','w') as tfile:\n",
    "    tfile.write(str(negatives))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
